{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ND4pgO3TjsDn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNBckaFdkMQp",
        "outputId": "f82dc1bd-e4bb-4de3-c505-bda3bcd7d55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Uk4N6giKkWlb",
        "outputId": "12088f07-6627-4bba-9651-46797ef7adab"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(x_train[0], cmap='gray') # imshow() function which simply displays an image.\n",
        "plt.show() # cmap is responsible for mapping a specific colormap to the values found in the array that you passed as the first argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQwUVA5Zkcrw",
        "outputId": "3617e386-2842-450c-d511-5595da3ef503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcJZjKiFkhoU",
        "outputId": "d093dc4c-a3ae-4cd1-cece-7f46f437ed02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape (60000, 28, 28)\n",
            "y_train shape (60000,)\n",
            "X_test shape (10000, 28, 28)\n",
            "y_test shape (10000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train shape\", x_train.shape)\n",
        "print(\"y_train shape\", y_train.shape)\n",
        "print(\"X_test shape\", x_test.shape)\n",
        "print(\"y_test shape\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WYsVFd7Mko5_"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lxwOIIysk8oA"
      },
      "outputs": [],
      "source": [
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255 # Each image has Intensity from 0 to 255\n",
        "x_test /= 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xesFMB9WlEx4"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "y_train = np.eye(num_classes)[y_train] # Return a 2-D array with ones on the diagonal and zeros elsewhere.\n",
        "y_test = np.eye(num_classes)[y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Jc2lSljilJmh"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu')) #returns a sequence of another vectors of dimension 512\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2z6DQALplSXQ"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', # for a multi-class classification problem\n",
        "optimizer=RMSprop(),\n",
        "metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_oC-hYGlS2v",
        "outputId": "535d6574-e57f-4c4c-fcb3-f4642f03d643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 8s 15ms/step - loss: 0.2562 - accuracy: 0.9209 - val_loss: 0.1022 - val_accuracy: 0.9680\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.1057 - accuracy: 0.9671 - val_loss: 0.0969 - val_accuracy: 0.9706\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 0.0744 - accuracy: 0.9770 - val_loss: 0.0653 - val_accuracy: 0.9802\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0582 - accuracy: 0.9821 - val_loss: 0.0689 - val_accuracy: 0.9798\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0458 - accuracy: 0.9857 - val_loss: 0.0678 - val_accuracy: 0.9814\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0403 - accuracy: 0.9872 - val_loss: 0.0643 - val_accuracy: 0.9818\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0340 - accuracy: 0.9894 - val_loss: 0.0703 - val_accuracy: 0.9810\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0310 - accuracy: 0.9899 - val_loss: 0.0715 - val_accuracy: 0.9824\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0246 - accuracy: 0.9918 - val_loss: 0.0667 - val_accuracy: 0.9834\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0234 - accuracy: 0.9927 - val_loss: 0.0708 - val_accuracy: 0.9835\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.0784 - val_accuracy: 0.9822\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.0783 - val_accuracy: 0.9832\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 0.0170 - accuracy: 0.9942 - val_loss: 0.0759 - val_accuracy: 0.9816\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0150 - accuracy: 0.9949 - val_loss: 0.0786 - val_accuracy: 0.9847\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0797 - val_accuracy: 0.9854\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.0123 - accuracy: 0.9960 - val_loss: 0.0782 - val_accuracy: 0.9848\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0104 - accuracy: 0.9964 - val_loss: 0.0752 - val_accuracy: 0.9840\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.0752 - val_accuracy: 0.9842\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.0748 - val_accuracy: 0.9858\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.0098 - accuracy: 0.9967 - val_loss: 0.0816 - val_accuracy: 0.9847\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128 # batch_size argument is passed to the layer to define a batch size for the inputs.\n",
        "epochs = 20\n",
        "history = model.fit(x_train, y_train,\n",
        "batch_size=batch_size,\n",
        "epochs=epochs,\n",
        "verbose=1, # verbose=1 will show you an animated progress bar eg. [==========]\n",
        "validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDOiqYwVm6nt",
        "outputId": "5839b89c-ae57-44e5-880f-4664c0d2c3f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.08159882575273514\n",
            "Test accuracy: 0.9847000241279602\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veyKlH9mm7S7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- This code snippet is an example of building a neural network model using TensorFlow and Keras to classify handwritten digits from the MNIST dataset. Let's go through it line by line:\n",
        "\n",
        "Libraries Imported\n",
        "import numpy as np: NumPy is used for numerical computations and array operations.\n",
        "from tensorflow.keras.models import Sequential: Sequential is a type of model in Keras where layers are added sequentially.\n",
        "from tensorflow.keras.layers import Dense, Dropout: Dense represents a fully connected layer, and Dropout is used for regularization.\n",
        "from tensorflow.keras.optimizers import RMSprop: RMSprop is an optimizer algorithm.\n",
        "from tensorflow.keras.datasets import mnist: This imports the MNIST dataset from Keras.\n",
        "import matplotlib.pyplot as plt: Matplotlib is used for plotting.\n",
        "from sklearn import metrics: Metrics from scikit-learn for evaluating the model.\n",
        "Loading and Visualizing Data\n",
        "mnist.load_data(): Loads the MNIST dataset into training and testing sets (x_train, y_train), (x_test, y_test).\n",
        "plt.imshow(x_train[0], cmap='gray'): Displays the first image from the training data.\n",
        "plt.show(): Shows the image using Matplotlib.\n",
        "print(x_train[0]): Prints the pixel values of the first image.\n",
        "Shape printing statements show the dimensions of the data arrays.\n",
        "Data Preprocessing\n",
        "x_train = x_train.reshape(60000, 784): Reshapes the training data to a 2D array (60000 samples, 784 features).\n",
        "x_test = x_test.reshape(10000, 784): Reshapes the testing data similarly.\n",
        "x_train = x_train.astype('float32') and x_test = x_test.astype('float32'): Converts the data type to float32.\n",
        "x_train /= 255 and x_test /= 255: Normalizes the pixel values to the range [0, 1].\n",
        "np.eye(num_classes)[y_train] and np.eye(num_classes)[y_test]: One-hot encodes the target labels (y_train and y_test).\n",
        "Model Definition and Compilation\n",
        "Sequential(): Initializes a sequential model.\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,))): Adds a dense layer with 512 neurons and ReLU activation as the first layer.\n",
        "model.add(Dropout(0.2)): Adds a dropout layer with a dropout rate of 0.2 for regularization.\n",
        "Similar layers are added for the second dense layer and output layer.\n",
        "model.compile(...): Compiles the model with categorical cross-entropy loss, RMSprop optimizer, and accuracy as the metric.\n",
        "Model Training\n",
        "model.fit(...): Trains the model on the training data for a specified number of epochs and batch size. Validation data is provided for evaluating the model's performance during training.\n",
        "Model Evaluation\n",
        "score = model.evaluate(x_test, y_test, verbose=0): Evaluates the model on the test data and computes the test loss and accuracy.\n",
        "The test loss and accuracy are printed out for evaluation.\n",
        "Overall, this code demonstrates the complete process of loading data, preprocessing, building, training, and evaluating a neural network model for image classification using the MNIST dataset. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " <!-- Group B Deep Learning\n",
        " Assignment No: 2A\n",
        " Title of the Assignment: Binary classification using Deep Neural Networks Example: Classify movie\n",
        " reviews into positive\" reviews and \"negative\" reviews, just based on the text content of the reviews.Use\n",
        " IMDB dataset\n",
        " Objective of the Assignment: Students should be able to Classify movie reviews into positive reviews\n",
        " and \"negative reviews on IMDB Dataset.\n",
        " Prerequisite:\n",
        " 1.\n",
        " 2.\n",
        " 3.\n",
        " Basic of programming language\n",
        " Concept of Classification\n",
        " Concept of Deep Neural Network--------------------------------------------------------------------------------------------------------------\n",
        "Contents for Theory:\n",
        " 1.\n",
        " 2.\n",
        " 3.\n",
        " 4.\n",
        " What is Classification\n",
        " Example of Classification\n",
        " How Deep Neural Network Work on Classification\n",
        " Code Explanation with Output--------------------------------------------------------------------------------------------------------------\n",
        "SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "What is Classification?\n",
        " Classification is a type of supervised learning in machine learning that involves categorizing data into\n",
        " predefined classes or categories based on a set of features or characteristics. It is used to predict the class\n",
        " of new, unseen data based on the patterns learned from the labeled training data.\n",
        " In classification, a model is trained on a labeled dataset, where each data point has a known class label.\n",
        " The model learns to associate the input features with the corresponding class labels and can then be used\n",
        " to classify new, unseen data.\n",
        " For example, we can use classification to identify whether an email is spam or not based on its content\n",
        " and metadata, to predict whether a patient has a disease based on their medical records and symptoms, or\n",
        " to classify images into different categories based on their visual features.\n",
        " Classification algorithms can vary in complexity, ranging from simple models such as decision trees and\n",
        " k-nearest neighbors to more complex models such as support vector machines and neural networks. The\n",
        " choice of algorithm depends on the nature of the data, the size of the dataset, and the desired level of\n",
        " accuracy and interpretability.\n",
        " Classification is a common task in deep neural networks, where the goal is to predict the class of an\n",
        " input based on its features. Here's an example of how classification can be performed in a deep neural\n",
        " network using the popular MNIST dataset of handwritten digits.\n",
        " The MNIST dataset contains 60,000 training images and 10,000 testing images of handwritten digits\n",
        " from 0 to 9. Each image is a grayscale 28x28 pixel image, and the task is to classify each image into one\n",
        " of the 10 classes corresponding to the 10 digits.\n",
        " We can use a convolutional neural network (CNN) to classify the MNIST dataset. A CNN is a type of\n",
        " deep neural network that is commonly used for image classification tasks.\n",
        " How Deep Neural Network Work on Classification\n",
        "Deep neural networks are commonly used for classification tasks because they can automatically learn to\n",
        " extract relevant features from raw input data and map them to the correct output class.\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "The basic architecture of a deep neural network for classification consists of three main parts: an input\n",
        " layer, one or more hidden layers, and an output layer. The input layer receives the raw input data, which\n",
        " is usually preprocessed to a fixed size and format. The hidden layers are composed of neurons that apply\n",
        " linear transformations and nonlinear activations to the input features to extract relevant patterns and\n",
        " representations. Finally, the output layer produces the predicted class labels, usually as a probability\n",
        " distribution over the possible classes.\n",
        " During training, the deep neural network learns to adjust its weights and biases in each layer to minimize\n",
        " the difference between the predicted output and the true labels. This is typically done by optimizing a\n",
        " loss function that measures the discrepancy between the predicted and true labels, using techniques such\n",
        " as gradient descent or stochastic gradient descent.\n",
        " One of the key advantages of deep neural networks for classification is their ability to learn hierarchical\n",
        " representations of the input data. In a deep neural network with multiple hidden layers, each layer learns\n",
        " to capture more complex and abstract features than the previous layer, by building on the representations\n",
        " learned by the earlier layers. This hierarchical structure allows deep neural networks to learn highly\n",
        " discriminative features that can separate different classes of input data, even when the data is highly\n",
        " complex or noisy.\n",
        " Overall, the effectiveness of deep neural networks for classification depends on the choice of\n",
        " architecture, hyperparameters, and training procedure, as well as the quality and quantity of the training\n",
        " data. When trained properly, deep neural networks can achieve state-of-the-art performance on a wide\n",
        " range of classification tasks, from image recognition to natural language processing.\n",
        " IMDB Dataset-The IMDB dataset is a large collection of movie reviews collected from the IMDB\n",
        " website, which is a popular source of user-generated movie ratings and reviews. The dataset consists of\n",
        " 50,000 movie reviews, split into 25,000 reviews for training and 25,000 reviews for testing.\n",
        " Each review is represented as a sequence of words, where each word is represented by an integer index\n",
        " based on its frequency in the dataset. The labels for each review are binary, with 0 indicating a negative\n",
        " review and 1 indicating a positive review.\n",
        " The IMDB dataset is commonly used as a benchmark for sentiment analysis and text classification tasks,\n",
        " where the goal is to classify the movie reviews as either positive or negative based on their text content.\n",
        " The dataset is challenging because the reviews are often highly subjective and can contain complex\n",
        " language and nuances of meaning, making it difficult for traditional machine learning approaches to\n",
        " accurately classify them.\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "Deeplearningapproaches,suchasdeepneuralnetworks,haveachievedstate-of-the-artperformanceon\n",
        " theIMDBdatasetbyautomaticallylearningtoextract relevant featuresfromtherawtextdataandmap\n",
        " themtothecorrectoutputclass.TheIMDBdataset iswidelyusedinresearchandeducationfornatural\n",
        " languageprocessingandmachinelearning,asitprovidesarichsourceof labeledtextdatafor training\n",
        " and testing deep learning models.\n",
        " Source Code and Output\n",
        "#TheIMDBsentimentclassificationdatasetconsistsof50,000moviereviewsfromIMDBusersthatare\n",
        " labeled as either positive (1) or negative (0).\n",
        " #Thereviewsarepreprocessedandeachoneisencodedasasequenceofwordindexesintheformof\n",
        " integers.\n",
        " #Thewordswithinthereviewsareindexedbytheiroverall frequencywithinthedataset.Forexample,\n",
        " the integer “2” encodes the second most frequent word in the data.\n",
        " # The 50,000 reviews are split into 25,000 for training and 25,000 for testing.\n",
        " # Text Process word by word at diffrent timestamp ( You may use RNN LSTM GRU )\n",
        " # convert input text to vector reprent input text\n",
        " # DOMAIN: Digital content and entertainment industry\n",
        " #CONTEXT:Theobjectiveof thisproject is tobuilda text classificationmodel that analyses the\n",
        " customer'ssentimentsbasedontheir reviews intheIMDBdatabase.Themodelusesacomplexdeep\n",
        " learningmodel to build an embedding layer followed by a classification algorithmtoanalyse the\n",
        " sentiment of the customers.\n",
        " #DATADESCRIPTION:TheDataset of 50,000movie reviews fromIMDB, labelledbysentiment\n",
        " (positive/negative).\n",
        " #Reviews have been preprocessed, and each review is encoded as a sequence ofword indexes\n",
        " (integers).\n",
        " #Forconvenience, thewordsare indexedbytheir frequencyinthedataset,meaningthefor thathas\n",
        " index 1 is the most frequent word.\n",
        " # Use the first 20 words from each review to speed up training, using a max vocabulary size of 10,000.\n",
        " #Asaconvention, \"0\"doesnotstandforaspecificword,but insteadisusedtoencodeanyunknown\n",
        " word.\n",
        " #PROJECTOBJECTIVE:Builda sequentialNLPclassifierwhichcanuse input textparameters to\n",
        " determine the customer sentiments.\n",
        " import numpy as np\n",
        " import pandas as pd\n",
        " from sklearn.model_selection import train_test_split\n",
        " #loading imdb data with most frequent 10000 words\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "from keras.datasets import imdb\n",
        " (X_train, y_train), (X_test, y_test)= imdb.load_data(num_words=10000)#youmaytake top10,000\n",
        " word frequently used review of movies other are discarded\n",
        " #consolidatingdataforEDAExploratorydataanalysis(EDA) isusedbydatascientiststoanalyzeand\n",
        " investigate data sets and summarize their main characteristics\n",
        " data=np.concatenate((X_train,X_test), axis=0)# axis0isfirst runningverticallydownwardsacross\n",
        " rows (axis 0), axis 1 is second running horizontally across columns (axis 1),\n",
        " label = np.concatenate((y_train, y_test), axis=0)\n",
        " X_train.shape\n",
        " (25000,)\n",
        " X_test.shape\n",
        " (25000,)\n",
        " y_train.shape\n",
        " (25000,)\n",
        " y_test.shape\n",
        " (25000,)\n",
        " print(\"Review is \",X_train[0])# series of no convertedword to vocabulory associated with index\n",
        " print(\"Review is \",y_train[0])\n",
        " Reviewis [1,194,1153,194,8255,78,228,5,6,1463,4369,5012,134,26,4,715,8,118,1634,14,\n",
        " 394,20,13,119,954,189,102,5,207,110,3103,21,14,69,188,8,30,23,7,4,249,126,93,4,114,\n",
        " 9,2300,1523,5,647,4,116,9,35,8163,4,229,9,340,1322,4,118,9,4,130,4901,19,4,1002,5,\n",
        " 89,29,952,46,37,4,455,9,45,43,38,1543,1905,398,4,1649,26,6853,5,163,11,3215,2,4,\n",
        " 1153,9,194,775,7,8255,2,349,2637,148,605,2,8003,15,123,125,68,2,6853,15,349,165,\n",
        " 4362,98,5,4,228,9,43,2,1157,15,299,120,5,120,174,11,220,175,136,50,9,4373,228,8255,\n",
        " 5,2,656,245,2350,5,4,9837,131,152,491,18,2,32,7464,1212,14,9,6,371,78,22,625,64,\n",
        " 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
        " Review is  0\n",
        " vocab=imdb.get_word_index()# Retrieve the word indexfile mapping words to indices\n",
        " print(vocab)\n",
        " {'fawn':34701, 'tsukino':52006, 'nunnery':52007, 'sonja':16816, 'vani':63951, 'woods':1408, 'spiders':\n",
        " 16115,\n",
        " y_train\n",
        " array([1, 0, 0, ..., 0, 1, 0])\n",
        " y_test\n",
        " array([0, 1, 1, ..., 0, 0, 0])\n",
        " # Function to perform relevant sequence adding on the data\n",
        " #Nowit is time toprepareourdata.Wewillvectorizeeveryreviewandfill itwithzerossothat it\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "contains exactly 10000 numbers.\n",
        " # That means we fill every review that is shorter than 500 with zeros.\n",
        " #Wedothisbecausethebiggestreviewisnearlythatlongandeveryinputforourneuralnetworkneeds\n",
        " to have the same size.\n",
        " # We also transform the targets into floats.\n",
        " # sequences is name of method the review less than 10000 we perform padding overthere\n",
        " # binary vectorization code:\n",
        " # VECTORIZE as one cannot feed integers into a NN\n",
        " # Encoding the integer sequences into a binary matrix - one hot encoder basically\n",
        " #Fromintegersrepresentingwords,atvariouslengths-toanormalizedonehotencodedtensor(matrix)\n",
        " of 10k columns\n",
        " defvectorize(sequences,dimension=10000): #Wewillvectorizeeveryreviewandfill itwithzeros\n",
        " so that it contains exactly 10,000 numbers.\n",
        " # Create an all-zero matrix of shape (len(sequences),dimension)\n",
        " results = np.zeros((len(sequences), dimension))\n",
        " for i, sequenceinenumerate(sequences):\n",
        " results[i, sequence] =1\n",
        " return results\n",
        " # Now we split our data into a training and a testing set.\n",
        " # The training set will contain  reviews and the testing set\n",
        " # # Set a VALIDATION set\n",
        " test_x = data[:10000]\n",
        " test_y = label[:10000]\n",
        " train_x = data[10000:]\n",
        " train_y = label[10000:]\n",
        " test_x.shape\n",
        " (10000,)\n",
        " test_y.shape\n",
        " (10000,)\n",
        " train_x.shape\n",
        " (40000,)\n",
        " train_y.shape\n",
        " (40000,)\n",
        " print(\"Categories:\", np.unique(label))\n",
        " print(\"Number of unique words:\",len(np.unique(np.hstack(data))))\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "# The hstack() function is used to stack arrays in sequence horizontally (column wise).\n",
        " Categories: [0 1]\n",
        " Number of unique words: 9998\n",
        " length = [len(i)foriindata]\n",
        " print(\"Average Review length:\", np.mean(length))\n",
        " print(\"Standard Deviation:\",round(np.std(length)))\n",
        " #Thewholedataset contains9998uniquewordsandtheaveragereviewlengthis234words,witha\n",
        " standard deviation of 173 words.\n",
        " Average Review length: 234.75892\n",
        " Standard Deviation: 173\n",
        " # If you look at the data you will realize it has been already pre-processed.\n",
        " # All words have been mapped to integers and the integers represent the words sorted by their frequency.\n",
        " # This is very common in text analysis to represent a dataset like this.\n",
        " # So 4 represents the 4th most used word,\n",
        " # 5 the 5th most used word and so on...\n",
        " # The integer 1 is reserved for the start marker,\n",
        " # the integer 2 for an unknown word and 0 for padding.\n",
        " # Let's look at a single training example:\n",
        " print(\"Label:\", label[0])\n",
        " Label: 1\n",
        " print(\"Label:\", label[1])\n",
        " Label: 0\n",
        " print(data[0])\n",
        " # Retrieves a dict mapping words to their index in the IMDB dataset.\n",
        " index = imdb.get_word_index() # word to index\n",
        " #Create invertedindexfromadictionarywithdocument idsaskeysandalistof termsasvaluesfor\n",
        " each document\n",
        " reverse_index =dict([(value, key)for(key, value)inindex.items()])# id to word\n",
        " decoded =\" \".join( [reverse_index.get(i -3,\"#\")foriindata[0]] )\n",
        " #Theindicesareoffsetby3because0,1and2arereservedindicesfor\"padding\",\"startofsequence\"\n",
        " and \"unknown\".\n",
        " print(decoded)\n",
        " #thisfilmwasjustbrilliantcastinglocationscenerystorydirectioneveryone'sreallysuitedthepartthey\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "playedandyoucouldjust imaginebeingthererobert#isanamazingactorandnowthesamebeing\n",
        " director # father came fromthe same scottish islandasmyself so i loved the fact therewasareal\n",
        " connection with this film the witty remarks throughout the film\n",
        " #Adding sequence to data\n",
        " #Vectorizationis theprocessofconvertingtextualdataintonumericalvectorsandisaprocessthat is\n",
        " usually applied once the text is cleaned.\n",
        " data = vectorize(data)\n",
        " label = np.array(label).astype(\"float32\")\n",
        " labelDF=pd.DataFrame({'label':label})\n",
        " sns.countplot(x='label', data=labelDF)\n",
        " # Creating train and test data set\n",
        " fromsklearn.model_selectionimporttrain_test_split\n",
        " X_train, X_test, y_train, y_test = train_test_split(data,label, test_size=0.20, random_state=1)\n",
        " X_train.shape\n",
        " (40000, 10000)\n",
        " X_test.shape\n",
        " (10000, 10000)\n",
        " # Let's create sequential model\n",
        " fromkeras.utilsimportto_categorical\n",
        " fromkerasimportmodels\n",
        " fromkerasimportlayers\n",
        " model = models.Sequential()\n",
        " # Input - Layer\n",
        " #Notethatweset theinput-shapeto10,000at theinput-layerbecauseourreviewsare10,000integers\n",
        " long.\n",
        " # The input-layer takes 10,000 as input and outputs it with a shape of 50.\n",
        " model.add(layers.Dense(50, activation =\"relu\", input_shape=(10000,)))\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "# Hidden - Layers\n",
        " #Pleasenoteyoushouldalwaysuseadropoutratebetween20%and50%.#hereinourcase0.3means\n",
        " 30% dropout we are using dropout to prevent overfitting.\n",
        " #Bytheway, ifyouwantyoucanbuildasentimentanalysiswithoutLSTMs, thenyousimplyneedto\n",
        " replace it by a flatten layer:\n",
        " model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        " model.add(layers.Dense(50, activation =\"relu\"))\n",
        " model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        " model.add(layers.Dense(50, activation =\"relu\"))\n",
        " # Output- Layer\n",
        " model.add(layers.Dense(1, activation =\"sigmoid\"))\n",
        " model.summary()\n",
        " Model: \"sequential\"\n",
        " _________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #\n",
        " =================================================================\n",
        " dense (Dense)               (None, 50)                500050\n",
        " dropout (Dropout)           (None, 50)                0\n",
        " dense_1 (Dense)             (None, 50)                2550\n",
        " dropout_1 (Dropout)         (None, 50)                0\n",
        " dense_2 (Dense)             (None, 50)                2550\n",
        " dense_3 (Dense)             (None, 1)                 51\n",
        " =================================================================\n",
        " Total params: 505,201\n",
        " Trainable params: 505,201\n",
        " Non-trainable params: 0\n",
        " #For early stopping\n",
        " # Stop training when a monitored metric has stopped improving.\n",
        " # monitor: Quantity to be monitored.\n",
        " # patience: Number of epochs with no improvement after which training will be\n",
        " stopped.\n",
        " import tensorflow as tf\n",
        " callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        " # We use the “adam” optimizer, an algorithm that changes the weights and biases\n",
        " during training.\n",
        " # We also choose binary-crossentropy as loss (because we deal with binary\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "classification) and accuracy as our evaluation metric.\n",
        " model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        " )\n",
        " from sklearn.model_selection import train_test_split\n",
        " results = model.fit(\n",
        " X_train, y_train,\n",
        " epochs= 2,\n",
        " batch_size = 500,\n",
        " validation_data = (X_test, y_test),\n",
        " callbacks=[callback]\n",
        " )\n",
        " # Let's check mean accuracy of our model\n",
        " print(np.mean(results.history[\"val_accuracy\"]))\n",
        " # Evaluate the model\n",
        " score = model.evaluate(X_test, y_test, batch_size=500)\n",
        " print('Test loss:', score[0])\n",
        " print('Test accuracy:', score[1])\n",
        " 20/20 [==============================]-1s 24ms/step-loss: 0.2511-accuracy:\n",
        " 0.8986\n",
        " Test loss: 0.25108325481414795\n",
        " Test accuracy: 0.8985999822616577\n",
        " #Let's plot training history of our model.\n",
        " # list all data in history\n",
        " print(results.history.keys())\n",
        " # summarize history for accuracy\n",
        " plt.plot(results.history['accuracy'])\n",
        " plt.plot(results.history['val_accuracy'])\n",
        " plt.title('model accuracy')\n",
        " plt.ylabel('accuracy')\n",
        " plt.xlabel('epoch')\n",
        " plt.legend(['train', 'test'], loc='upper left')\n",
        " plt.show()\n",
        " # summarize history for loss\n",
        " plt.plot(results.history['loss'])\n",
        " plt.plot(results.history['val_loss'])\n",
        " plt.title('model loss')\n",
        " plt.ylabel('loss')\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineering\n",
        "plt.xlabel('epoch')\n",
        " plt.legend(['train', 'test'], loc='upper left')\n",
        " plt.show()\n",
        " Conclusion- In this way we can Classify the MovieReviews by using DNN.\n",
        " Assignment Question\n",
        " 1. What is Binary Classification?\n",
        " 2. What is binary Cross Entropy?\n",
        " 3. What is Validation Split?\n",
        " 4. What  is the Epoch Cycle?\n",
        " 5. What is Adam Optimizer?\n",
        " SNJB’s Late Sau. K.B. Jain College Of Engineerin -->"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
